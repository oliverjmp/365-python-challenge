### ü§ñ Proyecto D√≠a 49: Recursive Web Crawler üï∑Ô∏èü™ú

Hoy implementamos un motor de navegaci√≥n secuencial. El script ya no es est√°tico; ahora es capaz de descubrir enlaces, "saltar" a nuevas URLs, extraer datos profundos y mantener un estado de recolecci√≥n persistente.

#### **Hitos T√©cnicos Alcanzados:**
1.  **L√≥gica de Crawling:** Implementaci√≥n de un flujo de "Landing -> Discovery -> Extraction".
2.  **Manejo de Cortes√≠a (Polite Scraping):** Introducci√≥n de `time.sleep()` para evitar sobrecargar servidores y prevenir bloqueos de IP.
3.  **Normalizaci√≥n de URLs:** Uso de `urljoin` para convertir rutas relativas (`/wiki/Python`) en rutas absolutas funcionales.
4.  **Recolecci√≥n Selectiva:** Filtrado de enlaces relevantes para evitar que el bot se pierda en secciones innecesarias (como men√∫s laterales o pies de p√°gina).

#### **Tecnolog√≠as Utilizadas:**
* **Requests & BeautifulSoup:** El n√∫cleo de conexi√≥n y parseo.
* **Time:** Para la gesti√≥n de intervalos entre peticiones.
* **Urllib.parse:** Para la reconstrucci√≥n inteligente de URLs.